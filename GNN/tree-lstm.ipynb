{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ca412aa-7e98-43e3-87bc-45305bee983c",
   "metadata": {},
   "source": [
    "# TreeLSTM\n",
    "https://github.com/stanfordnlp/treelstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1e271d-aac7-49c4-b033-3159e2f1a468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4de436cd-1a98-41cd-a31a-ff41ed4dd2f4",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5af959cd-18bc-4d5e-9e6e-f5d540f56ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ca2e9803-007c-4f2f-bd1e-e576635d1f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import math\n",
    "# import torchtext\n",
    "import datasets\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "49b0522c-14e9-4d25-8911-b06c2ac02dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f90ae813f50>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ad5604e4-d346-45ab-8acb-8d24546cc898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/kylehamilton/.cache/huggingface/datasets/Kyleiwaniec___parquet/Kyleiwaniec--PTC_Corpus-dc238a4991580b39/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d528f5bbb4bb4c20838000a0cfe5d8fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"Kyleiwaniec/PTC_Corpus\",use_auth_token='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')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171b50eb-6c23-4a78-a011-e3e4720660a7",
   "metadata": {},
   "source": [
    "## Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ba198f32-6085-45af-abd8-2d02822c121a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<benepar.integrations.spacy_plugin.BeneparComponent at 0x7f907430ae20>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize using spacy\n",
    "import benepar, spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from benepar import BeneparComponent, NonConstituentException\n",
    "# nlp.add_pipe(BeneparComponent(\"benepar_en3\"))\n",
    "nlp.add_pipe('benepar', config={'model': 'benepar_en3'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d6c94288-eb84-4156-a616-60940a43bf7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.0'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e495a8aa-ebce-4e58-a399-f7fa2ce1dad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://tedboy.github.io/nlps/generated/generated/nltk.ParentedTree.html\n",
    "import nltk\n",
    "from nltk import Tree, ParentedTree, CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6cc552d1-9ab7-44c7-95ca-4acee9dbabc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    tags = [token.tag_ for token in doc]\n",
    "    return (tokens, tags)\n",
    "# tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "608db69e-193c-4486-afd8-caabaec1ec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_string = '(S (SBAR (IN If) (S (NP (PRP he)) (VP (VBZ crosses)))) (, ,) (NP (PRP he)) (VP (VBZ joins) (NP (NNP Bush) (NNP II)) (PP (IN in) (NP (DT the) (NN history) (NNS books)))) (. .))'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1f1300c2-ce2e-48c2-83a8-ecc2f9bd7854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S',\n",
       " 'SBAR',\n",
       " 'IN',\n",
       " 'S',\n",
       " 'NP',\n",
       " 'PRP',\n",
       " 'VP',\n",
       " 'VBZ',\n",
       " ',',\n",
       " 'NP',\n",
       " 'PRP',\n",
       " 'VP',\n",
       " 'VBZ',\n",
       " 'NP',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'PP',\n",
       " 'IN',\n",
       " 'NP',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'NNS',\n",
       " '.']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p.split(\" \")[0] for p in parse_string.split('(') if len(p.split(\" \")[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a4e71d29-3de4-40af-bda5-11227d5f8f83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d09fc45d-521a-43cf-a8b7-7bf84be1d3c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "362ae87b-f847-43f7-b5ea-ee4b71d9b0f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CANNOT INSTALL TORCHTEXT #\n",
    "# tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "\n",
    "def tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    tags = [token.tag_ for token in doc]\n",
    "    parse_string = sent = list(doc.sents)[0]\n",
    "    parse_string = sent._.parse_string\n",
    "    constituents = [p.split(\" \")[0] for p in parse_string.split('(') if len(p.split(\" \")[0])]\n",
    "    return [tokens,tags,constituents]\n",
    "\n",
    "\n",
    "tokenize_data = lambda example, tokenizer: {'tokens': tokenize(example['text'])[0],\n",
    "                                            'tags': tokenize(example['text'])[1],\n",
    "                                            'constituents': tokenize(example['text'])[2]}  \n",
    "\n",
    "tokenized_sample = dataset['train'].select(range(10)).map(tokenize_data, fn_kwargs={'tokenizer': tokenizer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "696e8664-fe3f-4245-a00d-84ee60993f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'article_id': '701225819', 'text': 'South Florida Muslim Leader Sofian Zakkout’s David Duke Day', 'technique_classification': [], 'offsets': [], 'labels': 0, 'tokens': ['South', 'Florida', 'Muslim', 'Leader', 'Sofian', 'Zakkout', '’s', 'David', 'Duke', 'Day'], 'tags': ['NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'POS', 'NNP', 'NNP', 'NNP'], 'constituents': ['NP', 'NP', 'NML', 'NML', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'NNP', 'POS', 'NNP', 'NNP', 'NNP']}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_sample[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afd94c8-ad94-459c-bfaf-b8a212c6d2ef",
   "metadata": {},
   "source": [
    "## Generate the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2493619a-2fc1-4e9a-9ad8-faffc22dc59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n",
      "97\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "vocab = defaultdict(str)\n",
    "\n",
    "# l = ['a','b','c']\n",
    "# l.index('c')\n",
    "\n",
    "\n",
    "with open('pennTreeBankTags.txt','r') as _f:\n",
    "    lines = _f.readlines()\n",
    "    for idx,line in enumerate(lines):\n",
    "        vocab[line.strip()]=idx\n",
    "\n",
    "#vocab\n",
    "\"\"\"\n",
    "{'ADJP': 0,\n",
    " '-ADV': 1,\n",
    " 'ADVP': 2,\n",
    " '-BNF': 3,\n",
    "...\n",
    "}\n",
    "\"\"\"\n",
    "print(len(vocab))\n",
    "vocab['<unk>'] = 95\n",
    "vocab['<eos>'] = 96\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b166497d-1884-4a8e-bc7d-ca40b411842c",
   "metadata": {},
   "source": [
    "# CANNOT INSTALL TORCHTEXT #\n",
    "\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_dataset['train']['tokens'], min_freq=3) \n",
    "vocab.insert_token('<unk>', 0)            # insert (push) unknown token\n",
    "vocab.insert_token('<eos>', 1)            # insert end of sentence token (eop)\n",
    "vocab.set_default_index(vocab['<unk>'])   # So that when a token isn't found, it returns the index of the unk token\n",
    "print(len(vocab))                         # total number words in the vocabulary\n",
    "print(vocab.get_itos()[:10])              # first 10 tokens converted to strings from tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e91b52f-5cdf-4edc-8a09-96b8c6cb93c1",
   "metadata": {},
   "source": [
    "## Construct data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "df523085-a340-424d-a357-dd10f30ff473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which token to use:\n",
    "which_tokens = ['tokens','tags','constituents']\n",
    "t = which_tokens[1]\n",
    "\n",
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []                                                       # Merge everything into one gigantic document that we wish to model (all the tokens)\n",
    "    labels = []\n",
    "    for example in dataset:\n",
    "        if example[t]:                                              # if the example has tokens (not empty)\n",
    "            tokens = example[t].append('<eos>')                     # append <eos> at the end of the sentence\n",
    "            tokens = [vocab[token] for token in example[t]]         # convert tokens to indices\n",
    "            data.extend(tokens)                                     # append tokens to data\n",
    "            labels.extend([example['labels']])\n",
    "    \n",
    "    data = torch.LongTensor(data)                                   # convert data to tensor\n",
    "    print(data.shape)\n",
    "    num_batches = data.shape[0] // batch_size \n",
    "    data = data[:num_batches * batch_size]                          # We only need the first num_batches * batch_size elements\n",
    "    data = data.view(batch_size, num_batches)                       # Perceive the data as a matrix of batch_size rows and num_batches columns\n",
    "    print(\"2-data\",data.shape)\n",
    "    \n",
    "    labels = torch.LongTensor(labels) \n",
    "    print(labels.shape)\n",
    "    num_batches = labels.shape[0] // batch_size\n",
    "    \n",
    "    labels = labels[:num_batches * batch_size]\n",
    "    \n",
    "    labels = labels.view(batch_size, num_batches)\n",
    "    print('2-labels',labels.shape)\n",
    "    return data,labels\n",
    "\n",
    "#Notice that train_data[:, i] is the batch of next tokens for train_data[:, i - 1] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "5dd71543-5f59-4cbf-9b69-a0c06ca56da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([367840])\n",
      "2-data torch.Size([64, 5747])\n",
      "torch.Size([14434])\n",
      "2-labels torch.Size([64, 225])\n",
      "torch.Size([51748])\n",
      "2-data torch.Size([64, 808])\n",
      "torch.Size([2067])\n",
      "2-labels torch.Size([64, 32])\n",
      "torch.Size([96832])\n",
      "2-data torch.Size([64, 1513])\n",
      "torch.Size([4083])\n",
      "2-labels torch.Size([64, 63])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_data, train_labels = get_data(tokenized_dataset['train'], vocab, batch_size)\n",
    "valid_data, valid_labels = get_data(tokenized_dataset['validation'], vocab, batch_size)\n",
    "test_data,  test_labels  = get_data(tokenized_dataset['test'], vocab, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "f06409e4-2e7a-40e4-b5b8-434b47f86b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 225])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "5d458e58-6c79-4bfb-a3ab-6389e8fd7cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 1,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 1, 0, 0],\n",
       "        [1, 1, 0,  ..., 1, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 1]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "43a735ba-b775-4045-a6a7-ea9844944f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[31, 31, 31,  ..., 10, 32, 17],\n",
       "        [10, 31, 31,  ..., 29, 67, 17],\n",
       "        [29, 84, 96,  ..., 84, 96, 10],\n",
       "        ...,\n",
       "        [10, 29, 71,  ..., 31, 48, 67],\n",
       "        [10, 30, 61,  ..., 19,  4, 19],\n",
       "        [30, 17, 31,  ..., 31, 31, 67]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "832c736b-8326-484d-8c9e-aaf5a8fb6a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 5747])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc144ec-00fc-4bed-a954-3948dd2a2d16",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "de25d1c9-c7f1-49c1-9499-a7b8d8661f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, tie_weights):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                            hidden_dim, \n",
    "                            num_layers=num_layers, \n",
    "                            dropout=dropout_rate, \n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        # self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size) # binary classification\n",
    "        \n",
    "        if tie_weights:\n",
    "            assert embedding_dim == hidden_dim, 'If tying weights then embedding_dim must equal hidden_dim'\n",
    "            self.embedding.weight = self.fc.weight\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, src, hidden):\n",
    "        embedding = self.dropout(self.embedding(src))\n",
    "        output, hidden = self.lstm(embedding, hidden)          \n",
    "        output = self.dropout(output) \n",
    "        prediction = self.fc(output)\n",
    "        \n",
    "        print(prediction.shape)\n",
    "        \n",
    "        prediction = torch.squeeze(prediction, 1)\n",
    "        pred_out = torch.sigmoid(prediction)\n",
    "        \n",
    "        return pred_out, hidden\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_range_emb = 0.1\n",
    "        init_range_other = 1/math.sqrt(self.hidden_dim)\n",
    "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n",
    "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
    "        self.fc.bias.data.zero_()\n",
    "        for i in range(self.num_layers):\n",
    "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.embedding_dim,\n",
    "                    self.hidden_dim).uniform_(-init_range_other, init_range_other) \n",
    "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hidden_dim, \n",
    "                    self.hidden_dim).uniform_(-init_range_other, init_range_other) \n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        return hidden, cell\n",
    "\n",
    "    # We don't learn the hidden state so we can detach it from the computation graph\n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden, cell = hidden\n",
    "        hidden = hidden.detach()\n",
    "        cell = cell.detach()\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad1deae-8216-4efc-9bed-586c4c1d9f38",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "3b1ae6a6-f7ac-4c42-888f-05522ed65258",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 1024\n",
    "hidden_dim = 1024\n",
    "num_layers = 2\n",
    "dropout_rate = 0.65              \n",
    "tie_weights = True                  \n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3fda42-3217-47a0-bc32-60efc6d1ebff",
   "metadata": {},
   "source": [
    "### Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "fc51cd36-ef5a-43c3-9090-fb509d7eab93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 16,893,025 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, tie_weights).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b3bd23-13de-468d-bcfb-ff19adacb72f",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/15g7-HyNIT35GUXUZ-am3285XmaQB-SSE?usp=sharing#scrollTo=ttfv1WfVeg_E   \n",
    "What we did is called \"using a fixed backpropagation through time window\" and it's just one of the ways to deal with the problem that we can't have a batch of sequences with unequal lengths.\n",
    "\n",
    "Now because we haven't performed this step of breaking the dataset into \"batches of L-sequences\" we will define a function that given the index of the first batch of tokens in the batch returns the corresponding batch of sequences."
   ]
  },
  {
   "cell_type": "raw",
   "id": "659c851b-ea7b-4702-83c7-80a414920039",
   "metadata": {},
   "source": [
    "def get_batch(data, seq_len, num_batches, idx):\n",
    "    src = data[:, idx:idx+seq_len]                   \n",
    "    target = data[:, idx+1:idx+seq_len+1]             # The target is the src shifted by one batch\n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "7958eb87-ebaf-47a4-9358-c298f4dc9094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, labels, seq_len, num_batches, idx):\n",
    "    src = data[:, idx:idx+seq_len]                   \n",
    "    target = labels[:, idx:idx+seq_len] \n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8111205-6b3f-4292-bae7-09688b34d814",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "9fb33af9-1309-45e7-84c5-c31abf1d8be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, labels, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    \n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):  # The last batch can't be a src\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "\n",
    "        src, target = get_batch(data, labels, seq_len, num_batches, idx)\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "        prediction, hidden = model(src, hidden)                 # model output\n",
    "\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)   \n",
    "        target = target.reshape(-1)\n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f7739d-3f65-4bfd-97a2-50d86e9bd35a",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "7bec1258-d651-4a80-84c7-55845fdee3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, labels, criterion, batch_size, seq_len, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "            src, target = get_batch(data, labels, seq_len, num_batches, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            batch_size= src.shape[0]\n",
    "\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b3a765-6f46-4dcb-b510-743d37e60b94",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c0bd37f6-d75b-41c0-b135-c4c143f2b00d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                                                                                              | 0/114 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 50, 97])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|█▏                                                                                                                                    | 1/114 [00:03<06:31,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 50, 97])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|██▎                                                                                                                                   | 2/114 [00:06<06:12,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 50, 97])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|███▌                                                                                                                                  | 3/114 [00:09<06:02,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 50, 97])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 50, 97])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (3200) to match target batch_size (1600).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vt/g1t9ds3s01z05zs4qg2dngk80000gn/T/ipykernel_38710/1216838120.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vt/g1t9ds3s01z05zs4qg2dngk80000gn/T/ipykernel_38710/301681146.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, labels, optimizer, criterion, batch_size, seq_len, clip, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1165\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3013\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3014\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (3200) to match target batch_size (1600)."
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "seq_len = 50\n",
    "clip = 0.25\n",
    "saved = False\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "\n",
    "if saved:\n",
    "    model.load_state_dict(torch.load('best-val-lstm_lm.pt',  map_location=device))\n",
    "    test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
    "    print(f'Test Perplexity: {math.exp(test_loss):.3f}')\n",
    "else:\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = train(model, train_data, train_labels, optimizer, criterion, batch_size, seq_len, clip, device)\n",
    "        valid_loss = evaluate(model, valid_data, valid_labels, criterion, batch_size, seq_len, device)\n",
    "        \n",
    "        lr_scheduler.step(valid_loss)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'best-val-lstm_lm.pt')\n",
    "\n",
    "        print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "        print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fa13fb-b789-494e-93fc-2df4a0e08dfb",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fb42c8c7-51ba-4308-9b05-6485f5fd49c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    tokens = tokenize(prompt)[1]\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  # temperature is unfair\n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()     # take one sample from the distribution\n",
    "            \n",
    "            while prediction == vocab['<unk>']:\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if prediction == vocab['<eos>']:\n",
    "                break\n",
    "\n",
    "            indices.append(prediction)\n",
    "\n",
    "    # itos = vocab.get_itos()\n",
    "    ids_to_tokens = {v:k for k,v in vocab.items()}\n",
    "    \n",
    "    \n",
    "    tokens = [ids_to_tokens[i] for i in indices]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "70e71159-77e2-45ff-88cc-0ed6806d5a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "VB IN PRP VBZ IN DT NN IN NN , PRP MD VB VBN IN NN .\n",
      "\n",
      "0.7\n",
      "VB IN PRP VBZ IN DT RB JJ NN IN NN , PRP VBP IN NN JJ IN NNP .\n",
      "\n",
      "0.75\n",
      "VB IN PRP VBZ IN DT RB JJ NN IN NN , PRP VBP IN NN JJ IN NNP .\n",
      "\n",
      "0.8\n",
      "VB IN PRP VBZ IN DT RB JJ NN IN NN , PRP VBP IN NN JJ IN NNP .\n",
      "\n",
      "1.0\n",
      "VB IN PRP VBZ IN DT RB JJ NN IN NN , PRP VBP IN NN JJ IN NNP .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Think about'\n",
    "max_seq_len = 30\n",
    "seed = 0\n",
    "\n",
    "# convert the code above into a for loop\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed)\n",
    "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03556cd6-24e9-446b-8847-d3ce3cda8b85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd875c73-45e5-4fe4-ab6e-bb663b88b4a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
